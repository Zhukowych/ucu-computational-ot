{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with broadly defining what is Linear Programming and its main usage:\n",
    "\n",
    "## Linear Programming or Linear Optimization:\n",
    "\n",
    "Method for solving a specific kind of mathematical optimization problems, namely problems with the following setting:\n",
    "\n",
    "- Linear function Z that needs to be optimized (minimized/maximized);\n",
    "- A set of linear constraints (equalities/inequalities) on the solution;\n",
    "- Non-Negativity restrictions, if needed.\n",
    "\n",
    "A standard formulation of a linear programming problem would be:\n",
    "\n",
    "Find vector $\\mathbf{x}$ that maximizes $\\mathbf{c^Tx}$ subject to constraints $\\mathbf{Ax} <= \\mathbf{b}$ and $\\mathbf{x} >= 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Transport as a linear programming problem\n",
    "\n",
    "Let us consider the original Kantorovich problem:\n",
    "\n",
    "$$L_C(\\mathbf{a, b}) = \\min_{P\\in \\mathbf{U(a, b)}} \\langle P, C \\rangle$$\n",
    "\n",
    "$$\\mathbf{U(a, b)} = \\{P \\in \\mathbb{R}^{n*m}_{+}: P1_m = \\mathbf{a} \\:\\: \\& \\:\\: P^T1_m = \\mathbf{b}\\}$$\n",
    "\n",
    "To present this as a linear programming problem, as we already have the objective function $Z = \\langle P, C \\rangle$, we need to properly encode the constraints on $P$ as a matrix-vector equality. This can be achieved by:\n",
    "1) Constructing $\\mathbf{p} = vec(P)$, which basically means that we transform matrix $P$ into a vector by stacking its columns one over another;\n",
    "2) Constructing the matrix representing our constraint equations - it is done using the formula $A = \\bigl( \\begin{matrix}\\mathbf{1_m}\\otimes I_n\\\\ I_m \\otimes \\mathbf{1_n^T}\\end{matrix} \\bigr)$. $\\otimes$ in this formula corresponds to Kronecker's product, in which we multiply each element of the first matrix by the second matrix. As a result, $A \\in \\mathbb{R}^{(n+m)*nm}$ and encodes all $n+m$ contraints on each row and column of $P$.\n",
    "3) Combining the previous two steps, we get the system $A\\mathbf{p}= \\bigl[ \\begin{matrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{matrix} \\bigr]$. The only thing left to note is that $\\mathbb{p} \\in \\mathbb{R}^{nm}_{+}$.\n",
    "\n",
    "Therefore, Kantovich OT problem can be rewritten as:\n",
    "\n",
    "$$L_C(\\mathbf{a, b}) = \\min_{\\substack{A\\mathbf{p}= \\bigl[ \\begin{matrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{matrix} \\bigr] \\\\ p \\in \\mathbb{R}^{nm}_{+}}} \\mathbf{c^Tp}, \\:\\:\\:\\:\\: \\mathbf{c} = vec(C)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual problem\n",
    "\n",
    "As with the original OT problem, this linear programming representation also has a different representation. To derive it (or rather, a weak duality equivalent as presented in the book), let us first construct the Lagrangian:\n",
    "\n",
    "$$H(\\mathbf{h}) = \\min_{p \\in \\mathbb{R}^{nm}_{+}} \\mathbf{c^Tp} - \\mathbf{h^T}(A\\mathbf{p - q}), \\:\\:\\:\\:\\: \\mathbf{q} = \\bigl[ \\begin{matrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{matrix} \\bigr]$$\n",
    "\n",
    "This Lagrangian, due to relaxed constraints on $\\mathbf{p}$, is bound to be no bigger than $L_C(\\mathbf{a, b})$\n",
    "Therefore, to approach $L_C(\\mathbf{a, b})$ we need to maximize $H(\\mathbf{h})$:\n",
    "\n",
    "$$ L^1_C(\\mathbf{a, b}) = \\max_\\mathbf{h}H(\\mathbf{h}) = \\max_\\mathbf{h}\\mathbf{h^Tq} + \\min_{p \\in \\mathbb{R}^{nm}_{+}}(\\mathbf{c}-A^T\\mathbf{h})^T\\mathbf{p}$$\n",
    "\n",
    "The only part left to note from the above equation is that $(\\mathbf{c}-A^T\\mathbf{h})$ has to be non-negative - otherwise, min can reach negative infinity and spoil our calculations. So, final representation for the dual problem is:\n",
    "\n",
    "$$L^1_C(\\mathbf{a, b}) = \\max_{\\substack{\\mathbf{h} \\in \\mathbb{R}^{n+m} \\\\ \\mathbf{\\mathbf{c} \\geq A^T\\mathbf{h}}}}\\mathbf{h^Tq}$$\n",
    "\n",
    "Or, equivalently,\n",
    "\n",
    "$$L^1_C(\\mathbf{a, b}) = \\max_{\\substack{\\mathbf{h} \\in \\mathbb{R}^{nm+} \\\\ \\mathbf{\\mathbf{c} \\geq A^T\\mathbf{h}}}}\\bigl[ \\begin{matrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{matrix} \\bigr]^T\\mathbf{h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear programming solution methods\n",
    "\n",
    "### Simplex method:\n",
    "\n",
    "Intuition: All the constraints combined form a feasible region, where the optimal solution lies. As the objective function is linear, it is component-wise monotone, and therefore the optimal solution has to exist somewhere at the border of the feasible region, more specifically - one of the vertices formed by the intersection of constraints should have this optimal value. This method is therefore \"traveling\" over the edges of the feasible region while picking the most \"profitable\" direction so as to finally reach the solution.\n",
    "\n",
    "Algorithm (according to provided jupiter notebooks, bold text - my notes):\n",
    "\n",
    "1. Initialization: Start with an initial basic feasible solution. For the optimal transport problem, this could be an initial transportation plan that satisfies all supply and demand constraints.\n",
    "\n",
    "2. Optimality Test: Check if the current solution is optimal by examining the reduced costs. If all reduced costs are non-negative, the current solution is optimal.\n",
    "\n",
    "    **At this point during every iteration, we end up with an equation $Z - (a_1x_1 - a_1x_2 + ...) + (b_1y_1 + b_2y_2 + ...) = const$. Achieving the above condition would then mean that the first bracket is empty, and further changing any variable would only decrease $Z$.**\n",
    "\n",
    "3. Pivot Selection: If the current solution is not optimal, select an entering variable (one with a negative reduced cost) to enter the basis. This corresponds to increasing the amount transported along a certain route.\n",
    "\n",
    "    **Usually the variable with largest absolute cost is chosen, as that means it will affect the value more drastically and thus optimize it quicker.**\n",
    "\n",
    "4. Determine Leaving Variable: Determine the leaving variable by performing a ratio test to ensure feasibility. This involves finding which variable should leave the basis to maintain the feasibility of the solution.\n",
    "\n",
    "    **At this step we determine which of the constraint equations has the harshest restrictions over the chosen variable, and pick it and its corresponding base variable.**\n",
    "\n",
    "5. Update Solution: Perform a pivot operation to update the basic feasible solution. This involves adjusting the transportation quantities and updating the basis.\n",
    "\n",
    "    **Carry out operations similar to that of matrix RREF transformation - make the cofficient of the corresponding variable in the chosen constraint equal to 1, then via elementary row operations make it the only non-zero element in the column.** \n",
    "\n",
    "Repeat: Repeat the optimality test, pivot selection, and update steps until an optimal solution is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constraints_matrix(n, m):\n",
    "    A = np.zeros((n + m, n * m))\n",
    "    for i in range(n):\n",
    "        A[i, i * m:(i + 1) * m] = 1\n",
    "    for i in range(m):\n",
    "        A[n + i, i::m] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "def ot_lp_solver(cost_matrix, a, b):\n",
    "\n",
    "    cost_matrix = np.array(cost_matrix)\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = cost_matrix.flatten()\n",
    "    A_check = get_constraints_matrix(a.size, b.size)\n",
    "\n",
    "    expected_check = np.concatenate((a, b))\n",
    "\n",
    "    result = linprog(c, A_eq=A_check, b_eq=expected_check)\n",
    "\n",
    "    if result.success:\n",
    "        return result.fun\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed:\", result.message)\n",
    "\n",
    "print(ot_lp_solver([[2, 3], [5, 1]], [3, 2], [1, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "def ot_dual_solver(cost_matrix, a, b):\n",
    "    cost_matrix = np.array(cost_matrix)\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = cost_matrix.flatten()\n",
    "    A_check = get_constraints_matrix(a.size, b.size).transpose()\n",
    "\n",
    "    conc_result = -np.concatenate((a, b)).transpose() # negation because linprog minimizes\n",
    "\n",
    "    result = linprog(conc_result, A_ub=A_check, b_ub=c)\n",
    "\n",
    "    if result.success:\n",
    "        return -result.fun\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed:\", result.message)\n",
    "\n",
    "print(ot_dual_solver([[2, 3], [5, 1]], [3, 2], [1, 4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
